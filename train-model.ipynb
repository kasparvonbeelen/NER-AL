{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_data(path, tokenizer=None):\n",
    "    with open(path,'r') as input_text:\n",
    "        text = input_text.read().strip()\n",
    "    \n",
    "    docs = text.split('\\n\\n')\n",
    "    data = []\n",
    "    labels = set()\n",
    "    \n",
    "    for i,d in enumerate(docs):\n",
    "\n",
    "        record = dict()\n",
    "        record['id'] = i\n",
    "        elements = [e.split(' ') for e in d.split('\\n')]\n",
    "        record['words'] = [e[0] for e in elements]\n",
    "        record['ner_tags'] = [e[-1] for e in elements]\n",
    "       \n",
    "        \n",
    "        \n",
    "        labels.update(record['ner_tags'])\n",
    "        data.append(record)\n",
    "    labels2id = dict()\n",
    "    labels2id = {l:i+1 for i,l in enumerate(labels) if l != 'O'}\n",
    "    labels2id['O'] = 0\n",
    "    \n",
    "    print(labels2id)\n",
    "    for record in data:\n",
    "        record['ner_tags'] = [labels2id[l] for l in record['ner_tags']]\n",
    "    # for record in data:\n",
    "        \n",
    "    #     tokens,token_labels = [],[]\n",
    "    #     for word,label in zip(record['words'],record['ner_tags']):\n",
    "    #         word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    #         tokens.extend(word_tokens)\n",
    "    #         token_labels.extend([labels2id[label]]*len(word_tokens))\n",
    "    #     record['tokens'] = tokens\n",
    "    #     record['ner_tags'] = token_labels\n",
    "             \n",
    "    return data,labels2id\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-Object': 1, 'B-Object': 2, 'O': 0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data, label2id = read_conll_data('data/project-5-at-2023-06-13-13-05-b9955df3.conll', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-Object': 1, 'B-Object': 2, 'O': 0}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "data = datasets.Dataset.from_pandas(pd.DataFrame(data=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f479fc3cef42bab05ff751d3c8789a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 970\n",
       "})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data.train_test_split(test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 646,\n",
       " 'words': ['LATER',\n",
       "  'FROM',\n",
       "  'AMERICA',\n",
       "  '.',\n",
       "  '-',\n",
       "  '..',\n",
       "  'llinv',\n",
       "  'YORK',\n",
       "  ',',\n",
       "  'Oct.',\n",
       "  '7',\n",
       "  '(',\n",
       "  'Evening',\n",
       "  '.',\n",
       "  ')',\n",
       "  'Mr.',\n",
       "  'Stanton',\n",
       "  'reports',\n",
       "  'tha',\n",
       "  '-',\n",
       "  't',\n",
       "  'the',\n",
       "  'operations',\n",
       "  'are',\n",
       "  'progressing',\n",
       "  'satisfactorily',\n",
       "  'before',\n",
       "  'Riehn.s.ond',\n",
       "  'and',\n",
       "  'Petersburg',\n",
       "  '.',\n",
       "  'No',\n",
       "  'engagement',\n",
       "  'has',\n",
       "  'taken',\n",
       "  \"p'ace\",\n",
       "  \"'\",\n",
       "  'shy',\n",
       "  '.',\n",
       "  ',',\n",
       "  'Saturday',\n",
       "  '.',\n",
       "  'Both',\n",
       "  'belligerents',\n",
       "  'were',\n",
       "  'fortifying',\n",
       "  '.',\n",
       "  'It',\n",
       "  'is',\n",
       "  \"i'ported\",\n",
       "  'that',\n",
       "  'Lee',\n",
       "  'is',\n",
       "  'concentrating',\n",
       "  'on',\n",
       "  'Gr',\n",
       "  'trit',\n",
       "  \"'s\",\n",
       "  'left',\n",
       "  '.',\n",
       "  'Thn',\n",
       "  'federal',\n",
       "  'loss',\n",
       "  'north',\n",
       "  'and',\n",
       "  'south',\n",
       "  'of',\n",
       "  'the',\n",
       "  'James',\n",
       "  'River',\n",
       "  'on',\n",
       "  'Fridly',\n",
       "  'and',\n",
       "  'Saturday',\n",
       "  'Is',\n",
       "  'estimated',\n",
       "  'at',\n",
       "  '4,000',\n",
       "  '.',\n",
       "  'The',\n",
       "  'North‘rn',\n",
       "  'papers',\n",
       "  'contain',\n",
       "  'a',\n",
       "  'report',\n",
       "  'that',\n",
       "  'the',\n",
       "  'larger',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'Grant',\n",
       "  \"'s\",\n",
       "  'corps',\n",
       "  'has',\n",
       "  're-',\n",
       "  'crossed',\n",
       "  'from',\n",
       "  'the',\n",
       "  'north',\n",
       "  'side',\n",
       "  'of',\n",
       "  'the',\n",
       "  'James',\n",
       "  'River',\n",
       "  ',',\n",
       "  '_',\n",
       "  'but',\n",
       "  'state',\n",
       "  'that',\n",
       "  'he',\n",
       "  'retains',\n",
       "  'his',\n",
       "  'position',\n",
       "  'in',\n",
       "  'the',\n",
       "  'surrounding',\n",
       "  'forts',\n",
       "  '.',\n",
       "  'General',\n",
       "  'Lee',\n",
       "  'reports',\n",
       "  'that',\n",
       "  'in',\n",
       "  'the',\n",
       "  'engagement',\n",
       "  'south',\n",
       "  'of',\n",
       "  'Petersburg',\n",
       "  'the',\n",
       "  'Federals',\n",
       "  'broke',\n",
       "  'through',\n",
       "  'a',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'his',\n",
       "  'line',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Squirrel',\n",
       "  'Level',\n",
       "  'Road',\n",
       "  ',',\n",
       "  'but',\n",
       "  'were',\n",
       "  'driven',\n",
       "  'back',\n",
       "  'with',\n",
       "  'a',\n",
       "  'loss',\n",
       "  'of',\n",
       "  '400',\n",
       "  'prisoners',\n",
       "  '.',\n",
       "  'General',\n",
       "  'Hampton',\n",
       "  'then',\n",
       "  'drove',\n",
       "  'the',\n",
       "  'enemy',\n",
       "  ',',\n",
       "  'capturing',\n",
       "  '500',\n",
       "  'prisoners',\n",
       "  '.',\n",
       "  'Mr.',\n",
       "  'Stanton',\n",
       "  'reports',\n",
       "  'that',\n",
       "  'the',\n",
       "  'Confederates',\n",
       "  'in',\n",
       "  'Sher-',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'rear',\n",
       "  ',',\n",
       "  'after',\n",
       "  'capturing',\n",
       "  'Big',\n",
       "  'Shanty',\n",
       "  ',',\n",
       "  'were',\n",
       "  'pursued',\n",
       "  'and',\n",
       "  'driven',\n",
       "  'from',\n",
       "  'the',\n",
       "  'field',\n",
       "  'with',\n",
       "  'heavy',\n",
       "  'loqs',\n",
       "  ',',\n",
       "  'leaving',\n",
       "  'behind',\n",
       "  'them',\n",
       "  'their',\n",
       "  'dead',\n",
       "  'and',\n",
       "  'wounded',\n",
       "  '.',\n",
       "  'Unofficial',\n",
       "  'dis-',\n",
       "  'patches',\n",
       "  'report',\n",
       "  'that',\n",
       "  'the',\n",
       "  'Confederates',\n",
       "  'left',\n",
       "  '1,000',\n",
       "  'dead',\n",
       "  'and',\n",
       "  'wounded',\n",
       "  '.',\n",
       "  '.',\n",
       "  'Southern',\n",
       "  'papers',\n",
       "  'state',\n",
       "  'that',\n",
       "  'Hood',\n",
       "  'had',\n",
       "  'moved',\n",
       "  'his',\n",
       "  'army',\n",
       "  'thirty',\n",
       "  '-',\n",
       "  'five',\n",
       "  'miles',\n",
       "  'westward',\n",
       "  'without',\n",
       "  'molestation',\n",
       "  '.',\n",
       "  'Sheri-',\n",
       "  'dan',\n",
       "  'is',\n",
       "  'at',\n",
       "  'Harrisonburg',\n",
       "  '.',\n",
       "  'Guerillas',\n",
       "  'occasionally',\n",
       "  'inter-',\n",
       "  'cept',\n",
       "  'his',\n",
       "  'supply',\n",
       "  'trains',\n",
       "  '.',\n",
       "  'Early',\n",
       "  'reports',\n",
       "  ',',\n",
       "  'en',\n",
       "  'the',\n",
       "  '29th',\n",
       "  'ult',\n",
       "  '.',\n",
       "  ',',\n",
       "  'that',\n",
       "  'if',\n",
       "  'the',\n",
       "  \"peop'e\",\n",
       "  'of',\n",
       "  'Lynchburg',\n",
       "  'will',\n",
       "  'guard',\n",
       "  'against',\n",
       "  'raiding',\n",
       "  'he',\n",
       "  'is',\n",
       "  'in',\n",
       "  'a',\n",
       "  'position',\n",
       "  'to',\n",
       "  'check',\n",
       "  'Sheridan',\n",
       "  \"'s\",\n",
       "  'further',\n",
       "  'Advance',\n",
       "  '.',\n",
       "  '.',\n",
       "  'Ewing',\n",
       "  \"'s\",\n",
       "  'force',\n",
       "  'has',\n",
       "  'arrived',\n",
       "  'at',\n",
       "  'Rolla',\n",
       "  ',',\n",
       "  'Missouri',\n",
       "  ',',\n",
       "  'after',\n",
       "  'having',\n",
       "  'lost',\n",
       "  'three',\n",
       "  'hundred',\n",
       "  'prisoners',\n",
       "  '.',\n",
       "  'Price',\n",
       "  'is',\n",
       "  'reported',\n",
       "  'to',\n",
       "  'be',\n",
       "  'six',\n",
       "  'miles',\n",
       "  'west',\n",
       "  'of',\n",
       "  'Union',\n",
       "  'City',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Jefferson',\n",
       "  'City',\n",
       "  'road',\n",
       "  '.',\n",
       "  'It',\n",
       "  'is',\n",
       "  's',\n",
       "  '.pposed',\n",
       "  'that',\n",
       "  'he',\n",
       "  'purposes',\n",
       "  'attempting',\n",
       "  'the',\n",
       "  'capture',\n",
       "  'of',\n",
       "  'Jefferson',\n",
       "  'City',\n",
       "  'and',\n",
       "  'the',\n",
       "  'fiatiolishment',\n",
       "  'of',\n",
       "  'a',\n",
       "  'State',\n",
       "  'Government',\n",
       "  '.',\n",
       "  'It',\n",
       "  'is',\n",
       "  'reported',\n",
       "  'that',\n",
       "  'Price',\n",
       "  'attempted',\n",
       "  'to',\n",
       "  'cross',\n",
       "  'the',\n",
       "  '05a2e',\n",
       "  'River',\n",
       "  ',',\n",
       "  'but',\n",
       "  'was',\n",
       "  'p',\n",
       "  ':',\n",
       "  'evented',\n",
       "  'by',\n",
       "  'the',\n",
       "  'Federals',\n",
       "  '.',\n",
       "  'The',\n",
       "  'Pacific',\n",
       "  'Railroad',\n",
       "  ',',\n",
       "  'eight',\n",
       "  'mites',\n",
       "  'from',\n",
       "  'Jef-',\n",
       "  'ferson',\n",
       "  'City',\n",
       "  ',',\n",
       "  'has',\n",
       "  'been',\n",
       "  'burned',\n",
       "  '.',\n",
       "  'Seversl',\n",
       "  'train',\n",
       "  '.',\n",
       "  'h',\n",
       "  '-',\n",
       "  't.7een',\n",
       "  'St.',\n",
       "  'Louis',\n",
       "  'and',\n",
       "  'Hannibal',\n",
       "  'have',\n",
       "  'also',\n",
       "  'been',\n",
       "  'burned',\n",
       "  '.',\n",
       "  'Admiral',\n",
       "  'Farragut',\n",
       "  'reta',\n",
       "  'ni',\n",
       "  'the',\n",
       "  'command',\n",
       "  'at',\n",
       "  'Mobile',\n",
       "  '.',\n",
       "  'The',\n",
       "  'Macon',\n",
       "  'Telegraph',\n",
       "  'contains',\n",
       "  'a',\n",
       "  'speech',\n",
       "  'al7eged',\n",
       "  'to',\n",
       "  'have',\n",
       "  'been',\n",
       "  'delivered',\n",
       "  'by',\n",
       "  'Jefferson',\n",
       "  'Davis',\n",
       "  ',',\n",
       "  'asserting',\n",
       "  'that',\n",
       "  'Sherman.will',\n",
       "  'be',\n",
       "  'compelled',\n",
       "  'to',\n",
       "  're',\n",
       "  '-',\n",
       "  'eitact',\n",
       "  'the',\n",
       "  'retreat',\n",
       "  'of',\n",
       "  'Moscow',\n",
       "  ',',\n",
       "  'that',\n",
       "  'the',\n",
       "  'independence',\n",
       "  'of',\n",
       "  'the',\n",
       "  'South',\n",
       "  'would',\n",
       "  'be',\n",
       "  'established',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'enemy',\n",
       "  'defeated',\n",
       "  'if',\n",
       "  'half',\n",
       "  'the',\n",
       "  'absent',\n",
       "  'troops',\n",
       "  'returned',\n",
       "  'to',\n",
       "  'their',\n",
       "  'duty',\n",
       "  '.',\n",
       "  'Tho',\n",
       "  'Southern',\n",
       "  'press',\n",
       "  'is',\n",
       "  'dissatisfied',\n",
       "  'with',\n",
       "  'the',\n",
       "  'speech',\n",
       "  'and',\n",
       "  'dlbt',\n",
       "  'it',\n",
       "  ';',\n",
       "  'authenticity',\n",
       "  '.',\n",
       "  'In',\n",
       "  'accordance',\n",
       "  'with',\n",
       "  'n',\n",
       "  'resolution',\n",
       "  'passed',\n",
       "  'by',\n",
       "  'Conven-',\n",
       "  'tion',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Governor',\n",
       "  'of',\n",
       "  'Tennessee',\n",
       "  'has',\n",
       "  'issued',\n",
       "  'an',\n",
       "  'order',\n",
       "  'that',\n",
       "  '02',\n",
       "  '.',\n",
       "  '-1.7e',\n",
       "  '-',\n",
       "  'rbuns',\n",
       "  'voting',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Presidential',\n",
       "  'election',\n",
       "  'must',\n",
       "  'swear',\n",
       "  'to',\n",
       "  'oppose',\n",
       "  'an',\n",
       "  'armistice',\n",
       "  'and',\n",
       "  'peace',\n",
       "  'negotiations',\n",
       "  'with',\n",
       "  'armed',\n",
       "  'rebels',\n",
       "  'until',\n",
       "  'constitutional',\n",
       "  'laws',\n",
       "  'and',\n",
       "  'consti-',\n",
       "  'tutional',\n",
       "  'proclamations',\n",
       "  'are',\n",
       "  're',\n",
       "  '-',\n",
       "  'established',\n",
       "  'throughout',\n",
       "  'the',\n",
       "  'State',\n",
       "  '.',\n",
       "  'The',\n",
       "  'Governor',\n",
       "  'of',\n",
       "  'Tennessee',\n",
       "  'has',\n",
       "  'conscripted',\n",
       "  'white',\n",
       "  'men',\n",
       "  'and',\n",
       "  'negroes',\n",
       "  'between',\n",
       "  'eighteen',\n",
       "  'and',\n",
       "  'forty',\n",
       "  '-',\n",
       "  'five',\n",
       "  'years',\n",
       "  'of',\n",
       "  'age',\n",
       "  '.',\n",
       "  'NEW',\n",
       "  'YORK',\n",
       "  ',',\n",
       "  'Oct.',\n",
       "  '8',\n",
       "  '(',\n",
       "  'Morning',\n",
       "  '.',\n",
       "  ')',\n",
       "  'The',\n",
       "  'Fed',\n",
       "  '.',\n",
       "  'Tals',\n",
       "  'under',\n",
       "  'Burbridge',\n",
       "  'attacked',\n",
       "  'Saltville',\n",
       "  ',',\n",
       "  'Western',\n",
       "  'Virginia',\n",
       "  ',',\n",
       "  'and',\n",
       "  'were',\n",
       "  'badly',\n",
       "  'repulsed',\n",
       "  ',',\n",
       "  'retreating',\n",
       "  'in',\n",
       "  'confusion',\n",
       "  ',',\n",
       "  'punu',\n",
       "  ',',\n",
       "  'd',\n",
       "  'by',\n",
       "  'the',\n",
       "  'Confederates',\n",
       "  '.',\n",
       "  'A',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'Price',\n",
       "  \"'s\",\n",
       "  'forces',\n",
       "  'have',\n",
       "  'appeared',\n",
       "  'before',\n",
       "  'Jefferson',\n",
       "  'City',\n",
       "  '.',\n",
       "  'The',\n",
       "  'Governor',\n",
       "  'of',\n",
       "  'Georgia',\n",
       "  'has',\n",
       "  'declined',\n",
       "  'Sherman',\n",
       "  \"'s\",\n",
       "  'proposition',\n",
       "  'for',\n",
       "  'an',\n",
       "  'informal',\n",
       "  'peace',\n",
       "  'conference',\n",
       "  '.',\n",
       "  'Gold',\n",
       "  '100',\n",
       "  'prem',\n",
       "  '.',\n",
       "  'NEW',\n",
       "  '.YORK',\n",
       "  ',',\n",
       "  'Oct',\n",
       "  ':',\n",
       "  '8',\n",
       "  ',',\n",
       "  '(',\n",
       "  'Evening',\n",
       "  '.',\n",
       "  ')',\n",
       "  'The',\n",
       "  'Confederates',\n",
       "  'yesterday',\n",
       "  'attacked',\n",
       "  'butler',\n",
       "  \"'s\",\n",
       "  \"'\",\n",
       "  'fries',\n",
       "  ',',\n",
       "  'drove',\n",
       "  'in',\n",
       "  'Kautz',\n",
       "  ',',\n",
       "  'and',\n",
       "  'captured',\n",
       "  'his',\n",
       "  'artillery',\n",
       "  '.',\n",
       "  'They',\n",
       "  'then',\n",
       "  'attacked',\n",
       "  'Binaey',\n",
       "  ',',\n",
       "  'but',\n",
       "  'were',\n",
       "  'repulsed',\n",
       "  '.',\n",
       "  'Birney',\n",
       "  'recaptured',\n",
       "  'Kautz',\n",
       "  \"'s\",\n",
       "  'position',\n",
       "  ',',\n",
       "  'and',\n",
       "  'drove',\n",
       "  'the',\n",
       "  'ConfeLrates',\n",
       "  'to',\n",
       "  'their',\n",
       "  'inner',\n",
       "  'line',\n",
       "  'of',\n",
       "  'defence',\n",
       "  '.',\n",
       "  'Money',\n",
       "  'easy',\n",
       "  '.',\n",
       "  'Gold',\n",
       "  '90',\n",
       "  'premium',\n",
       "  '.',\n",
       "  'A',\n",
       "  'COTTON',\n",
       "  'MILL',\n",
       "  'BURNT',\n",
       "  \"'\",\n",
       "  'Dowss',\n",
       "  'NEAR',\n",
       "  'HALIZAX.-012',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'of',\n",
       "  'October',\n",
       "  '15',\n",
       "  'the',\n",
       "  'cotton',\n",
       "  'mill',\n",
       "  'near',\n",
       "  'Lamb',\n",
       "  'Bridge',\n",
       "  ',',\n",
       "  'Mist',\n",
       "  'B',\n",
       "  'ink',\n",
       "  ',',\n",
       "  'Halifax',\n",
       "  ',',\n",
       "  'was',\n",
       "  'burnt',\n",
       "  'to',\n",
       "  'the',\n",
       "  'ground',\n",
       "  '.',\n",
       "  'The',\n",
       "  'mill',\n",
       "  'was',\n",
       "  'in',\n",
       "  'the',\n",
       "  'occupation',\n",
       "  'of',\n",
       "  'Messrs.',\n",
       "  'Isleale',\n",
       "  ',',\n",
       "  'Booth',\n",
       "  ',',\n",
       "  'and',\n",
       "  'Co.',\n",
       "  'The',\n",
       "  'estimatsd',\n",
       "  'damage',\n",
       "  'to',\n",
       "  'the',\n",
       "  'machinery',\n",
       "  'and',\n",
       "  'stock',\n",
       "  'is',\n",
       "  'between',\n",
       "  \"'\",\n",
       "  '£',\n",
       "  '6,000',\n",
       "  'and',\n",
       "  '£',\n",
       "  '7,000',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'hui',\n",
       "  '!',\n",
       "  'ding',\n",
       "  '•',\n",
       "  ',',\n",
       "  'CHURCH',\n",
       "  'CoeEcTioxs.—On',\n",
       "  'Sunday',\n",
       "  ',',\n",
       "  'Octsber',\n",
       "  '16',\n",
       "  ',',\n",
       "  'after',\n",
       "  'two',\n",
       "  'sermons',\n",
       "  'for',\n",
       "  'church',\n",
       "  'expenses',\n",
       "  ',',\n",
       "  'by',\n",
       "  'a',\n",
       "  'well',\n",
       "  'known',\n",
       "  'incumbent',\n",
       "  'at',\n",
       "  'the',\n",
       "  'east',\n",
       "  'end',\n",
       "  'of',\n",
       "  'Losdon',\n",
       "  ',',\n",
       "  'the',\n",
       "  'following',\n",
       "  'items',\n",
       "  'were',\n",
       "  'found',\n",
       "  'in',\n",
       "  'the',\n",
       "  'plates',\n",
       "  ':',\n",
       "  '-7',\n",
       "  'cheques',\n",
       "  ',',\n",
       "  '6',\n",
       "  'sove-',\n",
       "  'reigns',\n",
       "  ',',\n",
       "  '15',\n",
       "  'half',\n",
       "  'sovereigns',\n",
       "  ',',\n",
       "  '1',\n",
       "  'crown',\n",
       "  'piece',\n",
       "  ',',\n",
       "  '36',\n",
       "  'half',\n",
       "  'crowns',\n",
       "  '17',\n",
       "  ',',\n",
       "  'florins',\n",
       "  ',',\n",
       "  '154',\n",
       "  'shillings',\n",
       "  ',',\n",
       "  '302',\n",
       "  'sixpences',\n",
       "  ',',\n",
       "  '63',\n",
       "  'fourperinfes',\n",
       "  ',',\n",
       "  '176',\n",
       "  'threepennies',\n",
       "  '171',\n",
       "  'pennies',\n",
       "  ',',\n",
       "  'and',\n",
       "  '151',\n",
       "  'halfpennies',\n",
       "  ',',\n",
       "  'making',\n",
       "  'a',\n",
       "  'total',\n",
       "  'of',\n",
       "  '1,099',\n",
       "  'pieces',\n",
       "  ',',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ag-',\n",
       "  'gregate',\n",
       "  'value',\n",
       "  'of',\n",
       "  '£',\n",
       "  '56',\n",
       "  'ss',\n",
       "  '.',\n",
       "  '60',\n",
       "  '.',\n",
       "  'CARLISLE',\n",
       "  'CA',\n",
       "  ',',\n",
       "  'TLE.—At',\n",
       "  'a',\n",
       "  'late',\n",
       "  'meeting',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Town',\n",
       "  'Council',\n",
       "  'of',\n",
       "  'C',\n",
       "  'the',\n",
       "  'Town',\n",
       "  'Clerk',\n",
       "  'reported',\n",
       "  'that',\n",
       "  'a',\n",
       "  'reply',\n",
       "  'had',\n",
       "  'been',\n",
       "  'received',\n",
       "  'from',\n",
       "  'the',\n",
       "  'War',\n",
       "  'Office',\n",
       "  ',',\n",
       "  'to',\n",
       "  'the',\n",
       "  'memorial',\n",
       "  'from',\n",
       "  'the',\n",
       "  'corporation',\n",
       "  'and',\n",
       "  'citizens',\n",
       "  ',',\n",
       "  'in',\n",
       "  'which',\n",
       "  'they',\n",
       "  'called',\n",
       "  'the',\n",
       "  'attention',\n",
       "  'of',\n",
       "  'the',\n",
       "  'authorities',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pro-',\n",
       "  'posed',\n",
       "  'desecration',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ancient',\n",
       "  'Castle',\n",
       "  'of',\n",
       "  'Carlisle',\n",
       "  ',',\n",
       "  'by',\n",
       "  'letting',\n",
       "  'a',\n",
       "  'large',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'it',\n",
       "  'for',\n",
       "  'trade',\n",
       "  'purposes',\n",
       "  ',',\n",
       "  'and',\n",
       "  'in',\n",
       "  'which',\n",
       "  'they',\n",
       "  'protested',\n",
       "  'against',\n",
       "  'this',\n",
       "  'course',\n",
       "  ';',\n",
       "  'at',\n",
       "  'the',\n",
       "  'same',\n",
       "  'time',\n",
       "  'the',\n",
       "  'reemorialists',\n",
       "  'suggested',\n",
       "  'that',\n",
       "  'a',\n",
       "  'fitting',\n",
       "  'and',\n",
       "  'not',\n",
       "  'Jishonourable',\n",
       "  'use',\n",
       "  'to',\n",
       "  'which',\n",
       "  'the',\n",
       "  'ancient',\n",
       "  'structure',\n",
       "  'might',\n",
       "  'be',\n",
       "  'put',\n",
       "  'would',\n",
       "  'be',\n",
       "  'to',\n",
       "  'constitute',\n",
       "  'it',\n",
       "  'the',\n",
       "  'head',\n",
       "  '-',\n",
       "  'quarters',\n",
       "  'of',\n",
       "  'the',\n",
       "  'county',\n",
       "  'militia',\n",
       "  '.',\n",
       "  'The',\n",
       "  'reply',\n",
       "  'which',\n",
       "  'had',\n",
       "  'been',\n",
       "  'received',\n",
       "  'stated',\n",
       "  'that',\n",
       "  'the',\n",
       "  'memorial',\n",
       "  'had',\n",
       "  'been',\n",
       "  'referred',\n",
       "  'to',\n",
       "  'Lord',\n",
       "  'Lonsdale',\n",
       "  ',',\n",
       "  'the',\n",
       "  'lord',\n",
       "  'lieutenant',\n",
       "  'of',\n",
       "  'the',\n",
       "  'county',\n",
       "  ',',\n",
       "  '\"',\n",
       "  'with',\n",
       "  '-whom',\n",
       "  'rests',\n",
       "  ',',\n",
       "  'under',\n",
       "  'the',\n",
       "  'provisions',\n",
       "  'of',\n",
       "  'the',\n",
       "  'law',\n",
       "  ',',\n",
       "  'to',\n",
       "  'approve',\n",
       "  'of',\n",
       "  'the',\n",
       "  'place',\n",
       "  'where',\n",
       "  'the',\n",
       "  'arms',\n",
       "  ',',\n",
       "  '&',\n",
       "  'c.',\n",
       "  ',',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Cumberland',\n",
       "  'militia',\n",
       "  'are',\n",
       "  'to',\n",
       "  'be',\n",
       "  'kept',\n",
       "  ...],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'input_ids': [101,\n",
       "  2101,\n",
       "  2013,\n",
       "  2637,\n",
       "  1012,\n",
       "  1011,\n",
       "  1012,\n",
       "  1012,\n",
       "  2222,\n",
       "  2378,\n",
       "  2615,\n",
       "  2259,\n",
       "  1010,\n",
       "  13323,\n",
       "  1012,\n",
       "  1021,\n",
       "  1006,\n",
       "  3944,\n",
       "  1012,\n",
       "  1007,\n",
       "  2720,\n",
       "  1012,\n",
       "  15845,\n",
       "  4311,\n",
       "  22794,\n",
       "  1011,\n",
       "  1056,\n",
       "  1996,\n",
       "  3136,\n",
       "  2024,\n",
       "  27673,\n",
       "  2938,\n",
       "  2483,\n",
       "  7011,\n",
       "  16761,\n",
       "  6588,\n",
       "  2077,\n",
       "  15544,\n",
       "  11106,\n",
       "  2078,\n",
       "  1012,\n",
       "  1055,\n",
       "  1012,\n",
       "  2006,\n",
       "  2094,\n",
       "  1998,\n",
       "  8062,\n",
       "  1012,\n",
       "  2053,\n",
       "  8147,\n",
       "  2038,\n",
       "  2579,\n",
       "  1052,\n",
       "  1005,\n",
       "  9078,\n",
       "  1005,\n",
       "  11004,\n",
       "  1012,\n",
       "  1010,\n",
       "  5095,\n",
       "  1012,\n",
       "  2119,\n",
       "  4330,\n",
       "  17071,\n",
       "  11187,\n",
       "  2020,\n",
       "  3481,\n",
       "  11787,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  1045,\n",
       "  1005,\n",
       "  27650,\n",
       "  2008,\n",
       "  3389,\n",
       "  2003,\n",
       "  16966,\n",
       "  2006,\n",
       "  24665,\n",
       "  13012,\n",
       "  2102,\n",
       "  1005,\n",
       "  1055,\n",
       "  2187,\n",
       "  1012,\n",
       "  16215,\n",
       "  2078,\n",
       "  2976,\n",
       "  3279,\n",
       "  2167,\n",
       "  1998,\n",
       "  2148,\n",
       "  1997,\n",
       "  1996,\n",
       "  2508,\n",
       "  2314,\n",
       "  2006,\n",
       "  10424,\n",
       "  3593,\n",
       "  2135,\n",
       "  1998,\n",
       "  5095,\n",
       "  2003,\n",
       "  4358,\n",
       "  2012,\n",
       "  1018,\n",
       "  1010,\n",
       "  2199,\n",
       "  1012,\n",
       "  1996,\n",
       "  2167,\n",
       "  1520,\n",
       "  29300,\n",
       "  4981,\n",
       "  5383,\n",
       "  1037,\n",
       "  3189,\n",
       "  2008,\n",
       "  1996,\n",
       "  3469,\n",
       "  4664,\n",
       "  1997,\n",
       "  3946,\n",
       "  1005,\n",
       "  1055,\n",
       "  3650,\n",
       "  2038,\n",
       "  2128,\n",
       "  1011,\n",
       "  4625,\n",
       "  2013,\n",
       "  1996,\n",
       "  2167,\n",
       "  2217,\n",
       "  1997,\n",
       "  1996,\n",
       "  2508,\n",
       "  2314,\n",
       "  1010,\n",
       "  1035,\n",
       "  2021,\n",
       "  2110,\n",
       "  2008,\n",
       "  2002,\n",
       "  14567,\n",
       "  2010,\n",
       "  2597,\n",
       "  1999,\n",
       "  1996,\n",
       "  4193,\n",
       "  15421,\n",
       "  1012,\n",
       "  2236,\n",
       "  3389,\n",
       "  4311,\n",
       "  2008,\n",
       "  1999,\n",
       "  1996,\n",
       "  8147,\n",
       "  2148,\n",
       "  1997,\n",
       "  8062,\n",
       "  1996,\n",
       "  2976,\n",
       "  2015,\n",
       "  3631,\n",
       "  2083,\n",
       "  1037,\n",
       "  4664,\n",
       "  1997,\n",
       "  2010,\n",
       "  2240,\n",
       "  2006,\n",
       "  1996,\n",
       "  18197,\n",
       "  2504,\n",
       "  2346,\n",
       "  1010,\n",
       "  2021,\n",
       "  2020,\n",
       "  5533,\n",
       "  2067,\n",
       "  2007,\n",
       "  1037,\n",
       "  3279,\n",
       "  1997,\n",
       "  4278,\n",
       "  5895,\n",
       "  1012,\n",
       "  2236,\n",
       "  11615,\n",
       "  2059,\n",
       "  5225,\n",
       "  1996,\n",
       "  4099,\n",
       "  1010,\n",
       "  11847,\n",
       "  3156,\n",
       "  5895,\n",
       "  1012,\n",
       "  2720,\n",
       "  1012,\n",
       "  15845,\n",
       "  4311,\n",
       "  2008,\n",
       "  1996,\n",
       "  24627,\n",
       "  1999,\n",
       "  2016,\n",
       "  2099,\n",
       "  1011,\n",
       "  2158,\n",
       "  1005,\n",
       "  1055,\n",
       "  4373,\n",
       "  1010,\n",
       "  2044,\n",
       "  11847,\n",
       "  2502,\n",
       "  17137,\n",
       "  3723,\n",
       "  1010,\n",
       "  2020,\n",
       "  9505,\n",
       "  1998,\n",
       "  5533,\n",
       "  2013,\n",
       "  1996,\n",
       "  2492,\n",
       "  2007,\n",
       "  3082,\n",
       "  8840,\n",
       "  4160,\n",
       "  2015,\n",
       "  1010,\n",
       "  2975,\n",
       "  2369,\n",
       "  2068,\n",
       "  2037,\n",
       "  2757,\n",
       "  1998,\n",
       "  5303,\n",
       "  1012,\n",
       "  11982,\n",
       "  4487,\n",
       "  2015,\n",
       "  1011,\n",
       "  13864,\n",
       "  3189,\n",
       "  2008,\n",
       "  1996,\n",
       "  24627,\n",
       "  2187,\n",
       "  1015,\n",
       "  1010,\n",
       "  2199,\n",
       "  2757,\n",
       "  1998,\n",
       "  5303,\n",
       "  1012,\n",
       "  1012,\n",
       "  2670,\n",
       "  4981,\n",
       "  2110,\n",
       "  2008,\n",
       "  7415,\n",
       "  2018,\n",
       "  2333,\n",
       "  2010,\n",
       "  2390,\n",
       "  4228,\n",
       "  1011,\n",
       "  2274,\n",
       "  2661,\n",
       "  15165,\n",
       "  2302,\n",
       "  16709,\n",
       "  20100,\n",
       "  1012,\n",
       "  2016,\n",
       "  3089,\n",
       "  1011,\n",
       "  4907,\n",
       "  2003,\n",
       "  2012,\n",
       "  6676,\n",
       "  4645,\n",
       "  1012,\n",
       "  19739,\n",
       "  11124,\n",
       "  25816,\n",
       "  5681,\n",
       "  6970,\n",
       "  1011,\n",
       "  8292,\n",
       "  13876,\n",
       "  2010,\n",
       "  4425,\n",
       "  4499,\n",
       "  1012,\n",
       "  2220,\n",
       "  4311,\n",
       "  1010,\n",
       "  4372,\n",
       "  1996,\n",
       "  16318,\n",
       "  17359,\n",
       "  2102,\n",
       "  1012,\n",
       "  1010,\n",
       "  2008,\n",
       "  2065,\n",
       "  1996,\n",
       "  21877,\n",
       "  7361,\n",
       "  1005,\n",
       "  1041,\n",
       "  1997,\n",
       "  11404,\n",
       "  4645,\n",
       "  2097,\n",
       "  3457,\n",
       "  2114,\n",
       "  23530,\n",
       "  2002,\n",
       "  2003,\n",
       "  1999,\n",
       "  1037,\n",
       "  2597,\n",
       "  2000,\n",
       "  4638,\n",
       "  13243,\n",
       "  1005,\n",
       "  1055,\n",
       "  2582,\n",
       "  5083,\n",
       "  1012,\n",
       "  1012,\n",
       "  24023,\n",
       "  1005,\n",
       "  1055,\n",
       "  2486,\n",
       "  2038,\n",
       "  3369,\n",
       "  2012,\n",
       "  4897,\n",
       "  2050,\n",
       "  1010,\n",
       "  5284,\n",
       "  1010,\n",
       "  2044,\n",
       "  2383,\n",
       "  2439,\n",
       "  2093,\n",
       "  3634,\n",
       "  5895,\n",
       "  1012,\n",
       "  3976,\n",
       "  2003,\n",
       "  2988,\n",
       "  2000,\n",
       "  2022,\n",
       "  2416,\n",
       "  2661,\n",
       "  2225,\n",
       "  1997,\n",
       "  2586,\n",
       "  2103,\n",
       "  2006,\n",
       "  1996,\n",
       "  7625,\n",
       "  2103,\n",
       "  2346,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  1055,\n",
       "  1012,\n",
       "  4903,\n",
       "  24768,\n",
       "  2008,\n",
       "  2002,\n",
       "  5682,\n",
       "  7161,\n",
       "  1996,\n",
       "  5425,\n",
       "  1997,\n",
       "  7625,\n",
       "  2103,\n",
       "  1998,\n",
       "  1996,\n",
       "  18550,\n",
       "  20282,\n",
       "  21808,\n",
       "  1997,\n",
       "  1037,\n",
       "  2110,\n",
       "  2231,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  2988,\n",
       "  2008,\n",
       "  3976,\n",
       "  4692,\n",
       "  2000,\n",
       "  2892,\n",
       "  1996,\n",
       "  5709,\n",
       "  2050,\n",
       "  2475,\n",
       "  2063,\n",
       "  2314,\n",
       "  1010,\n",
       "  2021,\n",
       "  2001,\n",
       "  1052,\n",
       "  1024,\n",
       "  2724,\n",
       "  2098,\n",
       "  2011,\n",
       "  1996,\n",
       "  2976,\n",
       "  2015,\n",
       "  1012,\n",
       "  1996,\n",
       "  3534,\n",
       "  4296,\n",
       "  1010,\n",
       "  2809,\n",
       "  10210,\n",
       "  2229,\n",
       "  2013,\n",
       "  15333,\n",
       "  2546,\n",
       "  1011,\n",
       "  10768,\n",
       "  17753,\n",
       "  2103,\n",
       "  1010,\n",
       "  2038,\n",
       "  2042,\n",
       "  5296,\n",
       "  1012,\n",
       "  7367,\n",
       "  14028,\n",
       "  2140,\n",
       "  3345,\n",
       "  1012,\n",
       "  1044,\n",
       "  1011,\n",
       "  1056,\n",
       "  1012,\n",
       "  1021,\n",
       "  12129,\n",
       "  2358,\n",
       "  1012,\n",
       "  3434,\n",
       "  1998,\n",
       "  24181,\n",
       "  2031,\n",
       "  2036,\n",
       "  2042,\n",
       "  5296,\n",
       "  1012,\n",
       "  5902,\n",
       "  2521,\n",
       "  29181,\n",
       "  4904,\n",
       "  2128,\n",
       "  2696,\n",
       "  9152,\n",
       "  1996,\n",
       "  3094,\n",
       "  2012,\n",
       "  4684,\n",
       "  1012,\n",
       "  1996,\n",
       "  20025,\n",
       "  10013,\n",
       "  3397,\n",
       "  1037,\n",
       "  4613,\n",
       "  2632,\n",
       "  2581,\n",
       "  24746,\n",
       "  2094,\n",
       "  2000,\n",
       "  2031,\n",
       "  2042,\n",
       "  5359,\n",
       "  2011,\n",
       "  7625,\n",
       "  4482,\n",
       "  1010,\n",
       "  27644,\n",
       "  2008,\n",
       "  11011,\n",
       "  1012,\n",
       "  2097,\n",
       "  2022,\n",
       "  15055,\n",
       "  2000,\n",
       "  2128,\n",
       "  1011,\n",
       "  1041,\n",
       "  6590,\n",
       "  6593,\n",
       "  1996,\n",
       "  7822,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I-Object', 'B-Object', 'O'], {'I-Object': 1, 'B-Object': 2, 'O': 0})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = list(label2id.keys())\n",
    "label_list,label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:l for l,i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-Object\",\n",
      "    \"2\": \"B-Object\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-Object\": 2,\n",
      "    \"I-Object\": 1,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/kasparbeelen/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/kasparbeelen/anaconda3/envs/ce/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 776\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 245\n",
      "  Number of trainable parameters = 66365187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b03a1ca4eb4fd18b10b6b199a0bd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 194\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b3ed2ab2304a77b52da45d226d836a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to object-detector/checkpoint-49\n",
      "Configuration saved in object-detector/checkpoint-49/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02587669901549816, 'eval_precision': 0.23076923076923078, 'eval_recall': 0.07125890736342043, 'eval_f1': 0.10889292196007261, 'eval_accuracy': 0.9923111985414068, 'eval_runtime': 23.9697, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 0.542, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/checkpoint-49/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/checkpoint-49/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/checkpoint-49/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 194\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be4e3e3d17b4332836ed18f3386678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to object-detector/checkpoint-98\n",
      "Configuration saved in object-detector/checkpoint-98/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008564875461161137, 'eval_precision': 0.8002577319587629, 'eval_recall': 0.7375296912114014, 'eval_f1': 0.7676143386897404, 'eval_accuracy': 0.9979848383072641, 'eval_runtime': 23.7108, 'eval_samples_per_second': 8.182, 'eval_steps_per_second': 0.548, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/checkpoint-98/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/checkpoint-98/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/checkpoint-98/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 194\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdd9d6e28f34b9ba13b859538ca5d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to object-detector/checkpoint-147\n",
      "Configuration saved in object-detector/checkpoint-147/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0066543458960950375, 'eval_precision': 0.8618090452261307, 'eval_recall': 0.8147268408551069, 'eval_f1': 0.8376068376068376, 'eval_accuracy': 0.9986445638614336, 'eval_runtime': 25.6452, 'eval_samples_per_second': 7.565, 'eval_steps_per_second': 0.507, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/checkpoint-147/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/checkpoint-147/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/checkpoint-147/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 194\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a8d270f0c3453da6b0c4c562259f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to object-detector/checkpoint-196\n",
      "Configuration saved in object-detector/checkpoint-196/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005791746079921722, 'eval_precision': 0.8766954377311961, 'eval_recall': 0.8444180522565321, 'eval_f1': 0.8602540834845736, 'eval_accuracy': 0.9988244890125708, 'eval_runtime': 22.6062, 'eval_samples_per_second': 8.582, 'eval_steps_per_second': 0.575, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/checkpoint-196/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, words, id. If ner_tags, words, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 194\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59086d9807f4a968b348c858aef5c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to object-detector/checkpoint-245\n",
      "Configuration saved in object-detector/checkpoint-245/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005650777369737625, 'eval_precision': 0.8777506112469438, 'eval_recall': 0.8527315914489311, 'eval_f1': 0.8650602409638554, 'eval_accuracy': 0.9988484790327223, 'eval_runtime': 22.981, 'eval_samples_per_second': 8.442, 'eval_steps_per_second': 0.566, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/checkpoint-245/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/checkpoint-245/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/checkpoint-245/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from object-detector/checkpoint-245 (score: 0.005650777369737625).\n",
      "Saving model checkpoint to object-detector\n",
      "Configuration saved in object-detector/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1956.328, 'train_samples_per_second': 1.983, 'train_steps_per_second': 0.125, 'train_loss': 0.027506915890440648, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in object-detector/pytorch_model.bin\n",
      "tokenizer config file saved in object-detector/tokenizer_config.json\n",
      "Special tokens file saved in object-detector/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"object-detector\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "checkpoint-147\n",
      "checkpoint-196\n",
      "checkpoint-245\n",
      "checkpoint-49\n",
      "checkpoint-98\n",
      "config.json\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "training_args.bin\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls object-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file object-detector/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"object-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-Object\",\n",
      "    \"2\": \"B-Object\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-Object\": 2,\n",
      "    \"I-Object\": 1,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file object-detector/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"object-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-Object\",\n",
      "    \"2\": \"B-Object\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-Object\": 2,\n",
      "    \"I-Object\": 1,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file object-detector/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at object-detector.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"object-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-Object',\n",
       "  'score': 0.9890133,\n",
       "  'index': 65,\n",
       "  'word': 'yard',\n",
       "  'start': 297,\n",
       "  'end': 301},\n",
       " {'entity': 'B-Object',\n",
       "  'score': 0.99073935,\n",
       "  'index': 124,\n",
       "  'word': 'body',\n",
       "  'start': 596,\n",
       "  'end': 600}]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(tokenized_data['test'][2]['words'])\n",
    "#text = 'I will take the train to London.'\n",
    "\n",
    "results = classifier(text)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file object-detector/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"object-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-Object\",\n",
      "    \"2\": \"B-Object\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-Object\": 2,\n",
      "    \"I-Object\": 1,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file object-detector/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"object-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-Object\",\n",
      "    \"2\": \"B-Object\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-Object\": 2,\n",
      "    \"I-Object\": 1,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file object-detector/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at object-detector.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def model_to_labelstudio(text, classifier):\n",
    "    results = classifier(text)\n",
    "\n",
    "    record = defaultdict(dict)\n",
    "    record['data']['text'] = text\n",
    "    record['data']['num_objects'] = len(results)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('data/project-5-at-2023-06-13-10-36-5850e0ef.conll').read().strip()\n",
    "docs = text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = docs[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE', '-X-', '_', 'O']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
